# Redis Destination Example

This example demonstrates how to use Artie Transfer with Redis as a destination.

## Prerequisites

- Redis server (6.0+)
- Kafka cluster
- Debezium connector (or another CDC source) sending data to Kafka

## Quick Start

### 1. Start Redis

Using Docker:
```bash
docker run -d -p 6379:6379 --name redis redis:latest
```

Or with password protection:
```bash
docker run -d -p 6379:6379 --name redis redis:latest redis-server --requirepass mypassword
```

### 2. Configure Transfer

Edit `config.yaml` and update:
- Redis connection settings (host, port, password, database)
- Kafka bootstrap server and topics
- Topic configurations for your tables

### 3. Run Transfer

```bash
./transfer run --config examples/redis/config.yaml
```

## Verifying Data

Connect to Redis and inspect the data:

```bash
# Connect to Redis
redis-cli

# List all streams
KEYS *

# Get stream info
XINFO STREAM mydb:public:users

# Read the last 10 entries from a stream
XREVRANGE mydb:public:users + - COUNT 10

# Read the first 10 entries
XRANGE mydb:public:users - + COUNT 10

# Get the length of a stream
XLEN mydb:public:users

# Read entries by ID
XRANGE mydb:public:users 1733136000000-0 1733136000000-0
```

Example output:
```
127.0.0.1:6379> XREVRANGE mydb:public:users + - COUNT 1
1) 1) "1733136123456-0"
   2) 1) "_artie_emitted_at"
      2) "2025-12-02T10:30:00Z"
      3) "_artie_data"
      4) "{\"id\":123,\"name\":\"John Doe\",\"email\":\"john@example.com\",\"created_at\":\"2025-01-01T00:00:00Z\"}"
```

## Data Model

Each CDC event creates a new entry in a Redis Stream with:
- **Stream key**: `[database]:[schema]:[table]`
- **Stream entry fields**:
  - `_artie_emitted_at`: When Transfer processed this record
  - `_artie_data`: Complete record as JSON
- **Stream ID**: Automatically generated by Redis (timestamp-sequence format)

### Benefits of Redis Streams

- **Ordered**: Entries are automatically ordered by insertion time
- **Efficient**: Better memory efficiency than individual hashes
- **Consumer Groups**: Support for multiple consumers with acknowledgments
- **Automatic Trimming**: Streams can be automatically limited to a max length
- **Time-Range Queries**: Query entries by time range using stream IDs

## Configuration Options

### Redis Settings

```yaml
redis:
  host: localhost          # Redis server hostname
  port: 6379              # Redis server port
  password: ""            # Optional password
  database: 0             # Redis database (0-15)
```

### Mode Settings

- **replication**: Real-time CDC replication
- **history**: Maintain complete history of changes

### Flush Settings

```yaml
flushIntervalSeconds: 10  # Flush every 10 seconds
flushSizeKb: 10240       # Flush when buffer reaches 10MB
bufferRows: 10000        # Maximum rows to buffer
```

## Use Cases

1. **Event Streaming**: Store CDC events for real-time processing with consumer groups
2. **Caching Layer**: Use Redis as a fast cache for recent changes
3. **Microservices Integration**: Multiple services can consume the same stream
4. **Analytics Pipeline**: Temporary storage before processing with automatic trimming
5. **Message Queue**: Durable event storage with Redis persistence and consumer acknowledgments
6. **Time-Series Data**: Query events by time range using stream IDs

## Monitoring

Monitor your Redis instance:

```bash
# Redis info
redis-cli INFO

# Memory usage
redis-cli INFO memory

# Number of keys
redis-cli DBSIZE

# Monitor live commands
redis-cli MONITOR
```

## Performance Tips

1. **Enable Redis persistence** (RDB or AOF) for durability
2. **Set maxmemory policy** to prevent OOM issues
3. **Monitor memory usage** and plan for growth
4. **Consider Redis Cluster** for horizontal scaling
5. **Use pipelining** (already implemented in Transfer)

## Data Retention

Redis Streams provide automatic trimming capabilities. The current implementation limits streams to 10,000 entries per stream (configurable via `streamMaxLen` constant in the code).

### Built-in Stream Trimming
The stream is automatically trimmed to the max length when new entries are added. This prevents unbounded growth.

### Consumer Groups
Create consumer groups to process stream data:
```bash
# Create a consumer group
XGROUP CREATE mydb:public:users mygroup $ MKSTREAM

# Read from the group
XREADGROUP GROUP mygroup consumer1 COUNT 10 STREAMS mydb:public:users >

# Acknowledge messages
XACK mydb:public:users mygroup 1733136123456-0
```

### Manual Cleanup
```bash
# Trim a stream to the last 1000 entries
XTRIM mydb:public:users MAXLEN ~ 1000

# Delete entire stream
DEL mydb:public:users
```

## Troubleshooting

### Connection Issues
```bash
# Test Redis connectivity
redis-cli -h localhost -p 6379 PING

# With password
redis-cli -h localhost -p 6379 -a mypassword PING
```

### Memory Issues
```bash
# Check memory usage
redis-cli INFO memory

# Get memory usage per key pattern
redis-cli --bigkeys
```

### Performance Issues
```bash
# Check slow queries
redis-cli SLOWLOG GET 10

# Monitor operations
redis-cli MONITOR
```

## Next Steps

- Configure multiple tables in `topicConfigs`
- Set up monitoring and alerts
- Implement data retention policies (stream trimming)
- Consider Redis Cluster for production use
- Add backup and recovery procedures
- **Try the consumer examples**: `consumer_example.sh` and `consumer_example.py`

## Consumer Examples

This directory includes example scripts for consuming CDC events from Redis Streams:

### Bash Consumer (`consumer_example.sh`)
```bash
# Make it executable
chmod +x consumer_example.sh

# Run the consumer
./consumer_example.sh
```

### Python Consumer (`consumer_example.py`)
```bash
# Install redis-py
pip install redis

# Run the consumer
python3 consumer_example.py
```

Both examples demonstrate:
- Creating consumer groups
- Reading messages from streams
- Processing CDC events
- Acknowledging messages
- Continuous consumption with blocking reads

